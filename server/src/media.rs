//! This module provides the [image] function, responsible for handling GET /image/.. requests, which may retrieve
//! either original media items or derived artifacts (e.g. thumbnails) for those items.  It also provides functions
//! and types useful for analyzing, resampling, reencoding, and deduplicating media items.

use {
    crate::{
        lock_map::LockMap,
        warp_util::{HttpDate, HttpError, Ranges},
        BUFFER_SIZE,
    },
    anyhow::{anyhow, Error, Result},
    bytes::{Bytes, BytesMut},
    futures::{stream, Stream, StreamExt, TryStreamExt},
    hex::FromHex,
    http::{header, status::StatusCode, Response},
    hyper::Body,
    image::{imageops::FilterType, GenericImageView, ImageFormat, Rgba},
    mp4parse::{CodecType, MediaContext, SampleEntry, Track, TrackType},
    rexiv2::{Metadata as ExifMetadata, Orientation},
    sqlx::SqliteConnection,
    std::{
        cmp::Ordering,
        collections::{HashMap, HashSet, VecDeque},
        convert::{TryFrom, TryInto},
        fmt::{self, Display},
        fs::File,
        hash::{Hash, Hasher},
        io::{Seek, SeekFrom, Write},
        mem,
        ops::{Deref, DerefMut},
        path::{Path, PathBuf},
        str::FromStr,
        sync::Arc,
        time::Duration,
    },
    tagger_shared::{Size, Variant},
    tempfile::NamedTempFile,
    tokio::{
        fs::{self, File as AsyncFile},
        io::{AsyncRead, AsyncReadExt, AsyncSeekExt},
        process::Command,
        sync::{Mutex as AsyncMutex, RwLock as AsyncRwLock},
        task,
    },
    tokio_util::codec::{BytesCodec, FramedRead},
};

/// Maximum bounding box for thumbnail images and videos
pub const SMALL_BOUNDS: (u32, u32) = (480, 320);

/// Maximum bounding box for high-resolution preview images and videos
pub const LARGE_BOUNDS: (u32, u32) = (1920, 1280);

/// Empirically derived quality setting used when encoding WEBP images
///
/// This provides a good tradeoff between file size and image quality, with no obvious artifacts in most cases.
const WEBP_QUALITY: f32 = 85.0;

/// Maximum length in hours:minutes:seconds of video thumbnails
///
/// The format of this string is what FFmpeg's "-to" option expects.
const VIDEO_PREVIEW_LENGTH_HMS: &str = "00:00:05";

/// Maximum absolute difference (averaged over each per-8-bit-color-channel of all pixels) permitted when comparing
/// two images in order to consider them duplicates of each other
const MAX_AVERAGE_ABSOLUTE_DIFFERENCE: usize = 5;

/// Width, in pixels, of resized image used to generate hash in [perceptual_hash]
const PERCEPTUAL_HASH_WIDTH: usize = 4;

/// Height, in pixels, of resized image used to generate hash in [perceptual_hash]
const PERCEPTUAL_HASH_HEIGHT: usize = 4;

/// Length, in bytes, of a hash generated by [perceptual_hash]
pub const PERCEPTUAL_HASH_LENGTH: usize = PERCEPTUAL_HASH_WIDTH * PERCEPTUAL_HASH_HEIGHT;

pub static MPEG4_EXTENSIONS: &[&str] = &[".mp4", ".mov", ".m4v"];

pub static JPEG_EXTENSIONS: &[&str] = &[".jpg", ".jpeg"];

/// Represents a media item
#[derive(Debug, Clone)]
pub struct Item {
    /// See [ItemData]
    pub data: ItemData,

    /// The perceptual hash for this media item
    ///
    /// See also [perceptual_hash].
    pub perceptual_hash: PerceptualHash,

    /// The name of the duplicate group, if any, to which this item has been assigned
    ///
    /// See also [deduplicate].
    pub duplicate_group: Option<String>,

    /// The index (i.e. quality ranking, where zero is best) of this item within its duplicate group, if any
    pub duplicate_index: Option<i64>,
}

impl Hash for Item {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.data.hash.hash(state);
    }
}

impl PartialEq for Item {
    fn eq(&self, other: &Self) -> bool {
        self.data.hash == other.data.hash
    }
}

impl Eq for Item {}

/// Pair of a media item hash and one of the files where that item was found
#[derive(Debug, Clone)]
pub struct ItemData {
    /// SHA-256 hash of the file in which this item is stored
    pub hash: String,

    /// See [FileData]
    pub file: FileData,
}

/// Metadata about a media item file
#[derive(Debug, Clone)]
pub struct FileData {
    /// Filesystem path where the file can be found
    pub path: String,

    /// Offset from the beginning of the file where MPEG-4 data can be found
    ///
    /// This will be zero for MP4 files, non-zero for "motion photo" files, and `None` for images with no embedded
    /// videos.
    pub video_offset: Option<i64>,
}

/// Find and return `FileData` for one of the files where the item with the specified `hash` was found.
///
/// There may be more than one such file in the database; this function will pick one arbitrarily.
async fn file_data(conn: &mut SqliteConnection, hash: &str) -> Result<FileData> {
    if let (Some(path), Some(image)) = (
        sqlx::query!("SELECT path FROM paths WHERE hash = ?1 LIMIT 1", hash)
            .fetch_optional(&mut *conn)
            .await?,
        sqlx::query!(
            "SELECT video_offset FROM images WHERE hash = ?1 LIMIT 1",
            hash
        )
        .fetch_optional(&mut *conn)
        .await?,
    ) {
        Ok(FileData {
            path: path.path,
            video_offset: image.video_offset,
        })
    } else {
        Err(HttpError::from_slice(StatusCode::NOT_FOUND, "not found").into())
    }
}

/// Return true iff the specified `orientation` is a 90 degree rotation in any direction.
fn orthagonal(orientation: Orientation) -> bool {
    matches!(
        orientation,
        Orientation::Rotate90HorizontalFlip
            | Orientation::Rotate90
            | Orientation::Rotate90VerticalFlip
            | Orientation::Rotate270
    )
}

/// Calculate the resized dimensions for an image using the specified bounding box, preserving aspect ratio (to
/// within integer precision) and respecting `orientation`.
fn bound(
    (native_width, native_height): (u32, u32),
    (mut bound_width, mut bound_height): (u32, u32),
    orientation: Orientation,
) -> (u32, u32) {
    if orthagonal(orientation) {
        mem::swap(&mut bound_width, &mut bound_height);
    }

    if native_width * bound_height > bound_width * native_height {
        (bound_width, (native_height * bound_width) / native_width)
    } else {
        ((native_width * bound_height) / native_height, bound_height)
    }
}

/// Produce a still image from the first frame of the specified video.
async fn still_image(image_dir: &str, path: &str) -> Result<(Vec<u8>, ImageFormat)> {
    let full_path = [image_dir, path].iter().collect::<PathBuf>();

    let lowercase = path.to_lowercase();

    if MPEG4_EXTENSIONS.iter().any(|&ext| lowercase.ends_with(ext)) {
        let output = Command::new("ffmpeg")
            .arg("-i")
            .arg(full_path)
            .arg("-ss")
            .arg("00:00:00")
            .arg("-frames:v")
            .arg("1")
            .arg("-f")
            .arg("mjpeg")
            .arg("-")
            .output()
            .await?;

        if output.status.success() {
            Ok((output.stdout, ImageFormat::Jpeg))
        } else {
            Err(anyhow!(
                "error running ffmpeg: {}",
                String::from_utf8_lossy(&output.stderr)
            ))
        }
    } else {
        Ok((
            content(&mut AsyncFile::open(full_path).await?).await?,
            ImageFormat::Jpeg,
        ))
    }
}

/// Parse the specified MPEG-4 file and extract its metadata.
async fn video_track_data(path: &Path, offset: i64) -> Result<VideoTrackData> {
    get_video_data(task::block_in_place(|| {
        let mut file = File::open(path)?;

        file.seek(SeekFrom::Start(offset.try_into().unwrap()))?;

        mp4parse::read_mp4(&mut file).map_err(Error::from)
    })?)
    .track_data
    .ok_or_else(|| {
        anyhow!(
            "unable to get video track data for {}",
            path.to_string_lossy()
        )
    })
}

/// Calculate an approximation of the average absolute difference between the bytes of the specified arrays.
fn average_absolute_difference(a: &[u8], b: &[u8], step: usize) -> usize {
    let (sum, count) = a.iter().step_by(step).zip(b.iter().step_by(step)).fold(
        (0, 0),
        |(sum, count), (&a, &b)| {
            (
                (sum + ((a as i32) - (b as i32)).unsigned_abs() as usize),
                count + 1,
            )
        },
    );

    sum / count
}

/// Produce a quality rating for the specified image, useful for comparing two duplicates and choosing the "best" one.
async fn quality(image_dir: &str, path: &str) -> Result<u64> {
    // Currently, we compute the quality as width*height.
    //
    // TODO: Investigate whether files contain metadata about encoding parameters and/or how many "generations" a
    // file is away from the original.

    let full_path = [image_dir, path].iter().collect::<PathBuf>();

    let lowercase = path.to_lowercase();

    let (width, height) = if MPEG4_EXTENSIONS.iter().any(|&ext| lowercase.ends_with(ext)) {
        video_track_data(&full_path, 0).await?.dimensions
    } else {
        // TODO: Can we easily get the dimensions without completely decoding the image?  Using `rexiv2`, perhaps?
        image::load_from_memory_with_format(
            &content(&mut AsyncFile::open(full_path).await?).await?,
            ImageFormat::Jpeg,
        )?
        .dimensions()
    };

    Ok(width as u64 * height as u64)
}

/// Group the specified media items according to transitive similarity.
//
/// `similar` represents a mapping such that each key item is considered "similar" to all of the value items (where
/// "similarity" is defined however the caller chooses).  This method will place each item in a group along with
/// any other items which it is either directly or indirectly similar to.
///
/// For example, if the input is {a: [b, c], b: [a, c], c: [a, b, d], d: [c], e: [f], g: []}, then the result
/// will be [{a, b, c, d}, {e, f}, {g}], but not necessarily in that order.
pub fn group_similar<'a>(similar: HashMap<&'a Item, HashSet<&'a Item>>) -> Vec<HashSet<&'a Item>> {
    let mut items_to_groups = HashMap::new();
    let mut groups_to_items = HashMap::new();
    let mut next_group = 0;

    for (item, others) in similar {
        if !items_to_groups.contains_key(item) {
            let group = next_group;
            next_group += 1;

            let mut set = HashSet::new();

            let mut add = |item, items_to_groups: &mut HashMap<_, _>| {
                set.insert(item);
                items_to_groups.insert(item, group);
            };

            add(item, &mut items_to_groups);

            let mut to_visit = others.clone();

            while let Some(other) = to_visit.iter().next().copied() {
                to_visit.remove(other);

                if let Some(&other_group) = items_to_groups.get(other) {
                    if other_group != group {
                        add(other, &mut items_to_groups);

                        if let Some(others) = groups_to_items.remove(&other_group) {
                            to_visit.extend(others);
                        }
                    }
                } else {
                    add(other, &mut items_to_groups);
                }
            }

            groups_to_items.insert(group, set);
        }
    }

    groups_to_items.into_values().collect()
}

/// Group `potential_duplicates` according to which ones appear to be duplicates of each other.
///
/// Each group in the result will be sorted from highest quality to lowest quality.
pub async fn deduplicate<'a>(
    image_locks: &LockMap<Arc<str>, ()>,
    image_dir: &str,
    cache_dir: &str,
    potential_duplicates: &HashSet<&'a Item>,
) -> Result<Vec<Vec<&'a Item>>> {
    if potential_duplicates.len() == 1 {
        return Ok(vec![potential_duplicates.iter().copied().collect()]);
    }

    // First, load the thumbnail for each item so we can compare them.

    let mut images = stream::iter(potential_duplicates)
        .then(|item| async move {
            Ok::<_, Error>((
                item,
                quality(image_dir, &item.data.file.path).await?,
                Vec::from(
                    &webp::Decoder::new(
                        &content(
                            &mut thumbnail(
                                Some(
                                    image_locks
                                        .get(Arc::from(item.data.hash.as_str()))
                                        .await
                                        .deref(),
                                ),
                                image_dir,
                                &item.data.file.path,
                                cache_dir,
                                Size::Small,
                                &item.data.hash,
                            )
                            .await?
                            .0,
                        )
                        .await?,
                    )
                    .decode()
                    .ok_or_else(|| anyhow!("unable to decode {}", &item.data.file.path))?
                        as &[u8],
                ),
            ))
        })
        .try_collect::<VecDeque<_>>()
        .await?;

    // Next, compare each image to each other one and group together any clusters of similar images.
    //
    // Note that we use task::block_in_place here since we need to do n^2 comparisons, each comparison is
    // expensive, and we don't want to to block the async runtime while we're doing it.
    task::block_in_place(|| {
        let mut similar = HashMap::<&Item, HashSet<&Item>>::new();
        let mut qualities = HashMap::new();

        // For speed, look at only every Nth sample
        let step = 17;

        while let Some((item, quality, image)) = images.pop_front() {
            qualities.insert(item, quality);

            similar.entry(item).or_default();

            for (other, _, other_image) in &images {
                if average_absolute_difference(&image, other_image, step)
                    <= MAX_AVERAGE_ABSOLUTE_DIFFERENCE
                {
                    similar.entry(item).or_default().insert(other);

                    similar.entry(other).or_default().insert(item);
                }
            }
        }

        Ok(group_similar(similar)
            .into_iter()
            .map(|items| {
                let mut items = items.into_iter().collect::<Vec<_>>();

                items.sort_by(|a, b| qualities.get(b).cmp(&qualities.get(a)));

                items
            })
            .collect())
    })
}

/// Perceptual ordinal for a media item
///
/// This is useful for sorting and grouping media items prior to deduplication.  See also `PerceptualHash::ordinal`
/// and `PerceptualOrdinal::is_similar_to`.
#[derive(Copy, Clone, Debug, Ord, PartialOrd, Eq, PartialEq)]
pub struct PerceptualOrdinal {
    /// Length of the video (if this item is a video), in seconds
    video_length_seconds: Option<u64>,

    /// Perceptual ordinal of the image (or first frame of the video)
    image_ordinal: u8,
}

impl PerceptualOrdinal {
    /// Indicates whether the two `PerceptualOrdinal`s are similar enough that the corresponding `PerceptualHash`es
    /// from which they were derived might also be similar, in which case the items they represent might be
    /// duplicates of each other.
    ///
    /// In other words, if the `PerceptualOrdinal`s for two items are *not* similar, then their `PerceptualHash`es
    /// are guaranteed to not be similar, and the items are guaranteed to not be duplicates according to the
    /// [deduplicate] function.  On the other hand, if the `PerceptualOrdinal`s are similar, then the
    /// `PerceptualHash`es *might* be similar, and if they are then the items *might* be duplicates.
    ///
    /// When items are sorted by their ordinal, the set of images with similar ordinals (if any) to that of a given
    /// item are guaranteed to be a contiguous range, i.e. once the first non-similar ordinal is found while
    /// iterating forward or backward from the item, there is no need to continue iterating.
    pub fn is_similar_to(&self, other: &PerceptualOrdinal) -> bool {
        self.video_length_seconds == other.video_length_seconds
            && match self.image_ordinal.cmp(&other.image_ordinal) {
                Ordering::Equal => true,
                Ordering::Less => {
                    usize::from(other.image_ordinal - self.image_ordinal)
                        <= MAX_AVERAGE_ABSOLUTE_DIFFERENCE
                }
                Ordering::Greater => {
                    usize::from(self.image_ordinal - other.image_ordinal)
                        <= MAX_AVERAGE_ABSOLUTE_DIFFERENCE
                }
            }
    }
}

/// Perceptual hash for a media item as calculated by [perceptual_hash]
#[derive(Clone, Debug)]
pub struct PerceptualHash {
    /// Length of the video (if this item is a video), in seconds
    pub video_length_seconds: Option<u64>,

    /// Perceptual hash of the image (or first frame of the video)
    pub image_hash: Vec<u8>,
}

impl PerceptualHash {
    /// Return `true` iff `self` is considered similar to `other`, i.e. if their `video_length_seconds` fields
    /// match and the average absolute difference between their `image_ordinal` fields, is less than a certain
    /// epsilon value.
    ///
    /// If the `PerceptualHash`es for two media items are not similar, then they are guaranteed to *not* be
    /// duplicates according to the [deduplicate] function.  On the other hand, if the `PerceptualHash`es are
    /// similar, then the items *might* be duplicates.  See also `PerceptualOrdinal::is_similar_to`.
    pub fn is_similar_to(&self, other: &PerceptualHash) -> bool {
        if self.video_length_seconds == other.video_length_seconds {
            average_absolute_difference(&self.image_hash, &other.image_hash, 1)
                <= MAX_AVERAGE_ABSOLUTE_DIFFERENCE
        } else {
            false
        }
    }

    pub fn ordinal(&self) -> PerceptualOrdinal {
        PerceptualOrdinal {
            video_length_seconds: self.video_length_seconds,
            image_ordinal: u8::try_from(
                self.image_hash
                    .iter()
                    .map(|&n| usize::from(n))
                    .sum::<usize>()
                    / self.image_hash.len(),
            )
            .unwrap(),
        }
    }
}

impl Display for PerceptualHash {
    /// Format a `PerceptualHash` as a string.
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        if let Some(length) = self.video_length_seconds {
            write!(f, "{length}-")?;
        }

        for v in &self.image_hash {
            write!(f, "{v:02x}")?;
        }

        Ok(())
    }
}

impl FromStr for PerceptualHash {
    type Err = Error;

    /// Parse an `PerceptualHash` from a string of the form generated by `PerceptualHash::fmt`.
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let mut split = s.split('-');

        Ok(if let (Some(a), Some(b)) = (split.next(), split.next()) {
            PerceptualHash {
                video_length_seconds: Some(a.parse()?),
                image_hash: Vec::from_hex(b)?,
            }
        } else {
            PerceptualHash {
                video_length_seconds: None,
                image_hash: Vec::from_hex(s)?,
            }
        })
    }
}

/// Calculate the perceptual hash of the media item at the specified `path`.
///
/// The resulting value may be used to group items by similarity prior to deduplicating them using [deduplicate].
pub async fn perceptual_hash(
    image_lock: &AsyncRwLock<()>,
    image_dir: &str,
    cache_dir: &str,
    hash: &str,
    path: &str,
    video_offset: Option<i64>,
) -> Result<PerceptualHash> {
    let image_hash = webp::Decoder::new(
        &content(
            &mut thumbnail(
                Some(image_lock),
                image_dir,
                path,
                cache_dir,
                Size::Small,
                hash,
            )
            .await?
            .0,
        )
        .await?,
    )
    .decode()
    .ok_or_else(|| anyhow!("unable to decode {path}"))?
    .to_image()
    .resize_exact(
        PERCEPTUAL_HASH_WIDTH.try_into().unwrap(),
        PERCEPTUAL_HASH_HEIGHT.try_into().unwrap(),
        FilterType::Lanczos3,
    )
    .pixels()
    .map(|(_, _, Rgba([r, g, b, _]))| {
        u8::try_from(((r as u32) + (g as u32) + (b as u32)) / 3).unwrap()
    })
    .collect();

    Ok(PerceptualHash {
        video_length_seconds: if video_offset == Some(0) {
            Some(
                video_track_data(&[image_dir, path].iter().collect::<PathBuf>(), 0)
                    .await?
                    .duration
                    .as_secs(),
            )
        } else {
            None
        },
        image_hash,
    })
}

/// Generate any missing thumbnail and preview artifacts for each of the specified items.
///
/// If an error is produced while generating artifacts for a given item, that error will be logged and this
/// function will continue on to the next item.
///
/// See also [preload_cache].
pub async fn preload_cache_all(
    image_locks: &LockMap<Arc<str>, ()>,
    image_dir: &str,
    cache_dir: &str,
    mut images: impl Stream<Item = Result<ItemData, sqlx::Error>> + Unpin,
) -> Result<Vec<(ItemData, Error)>> {
    let mut errors = Vec::new();

    while let Some(item) = images.try_next().await? {
        if let Err(e) = preload_cache(
            image_locks.get(Arc::from(item.hash.as_str())).await.deref(),
            image_dir,
            &item.file,
            cache_dir,
            &item.hash,
        )
        .await
        {
            tracing::warn!("error preloading cache for {}: {e:?}", item.hash);

            errors.push((item, e));
        }
    }

    Ok(errors)
}

/// Generate any missing thumbnail and preview artifacts for each of the specified item.
pub async fn preload_cache(
    image_lock: &AsyncRwLock<()>,
    image_dir: &str,
    file_data: &FileData,
    cache_dir: &str,
    hash: &str,
) -> Result<()> {
    for size in &[Size::Small, Size::Large] {
        get_variant(
            image_lock,
            image_dir,
            file_data,
            cache_dir,
            Variant::Still(*size),
            hash,
        )
        .await?;

        if file_data.video_offset.is_some() {
            get_variant(
                image_lock,
                image_dir,
                file_data,
                cache_dir,
                Variant::Video(*size),
                hash,
            )
            .await?;
        }
    }

    Ok(())
}

/// Return a handle to the file containing the requested artifact, which will be generated on the fly from the
/// original media item if it doesn't already exist.
///
/// The return value is a tuple containing the file handle, MIME type, and length.
async fn get_variant(
    image_lock: &AsyncRwLock<()>,
    image_dir: &str,
    file_data: &FileData,
    cache_dir: &str,
    variant: Variant,
    hash: &str,
) -> Result<(AsyncFile, &'static str, u64)> {
    match variant {
        Variant::Still(size) => {
            let (thumbnail, length, _) = thumbnail(
                Some(image_lock),
                image_dir,
                &file_data.path,
                cache_dir,
                size,
                hash,
            )
            .await?;

            Ok((thumbnail, "image/webp", length))
        }

        Variant::Video(size) => {
            if let Some(offset) = file_data.video_offset {
                let (preview, length) = video_preview(
                    image_lock,
                    image_dir,
                    &file_data.path,
                    offset,
                    cache_dir,
                    size,
                    hash,
                )
                .await?;

                Ok((preview, "video/mp4", length))
            } else {
                Err(HttpError::from_slice(StatusCode::NOT_FOUND, "not found").into())
            }
        }

        Variant::Original => {
            let lowercase = file_data.path.to_lowercase();

            let file =
                AsyncFile::open([image_dir, &file_data.path].iter().collect::<PathBuf>()).await?;
            let length = file.metadata().await?.len();

            Ok((
                file,
                if MPEG4_EXTENSIONS.iter().any(|&ext| lowercase.ends_with(ext)) {
                    "video/mp4"
                } else {
                    "image/jpeg"
                },
                length,
            ))
        }
    }
}

/// Collect the contents of the specified file into a `Vec<u8>` and return it.
async fn content(file: &mut AsyncFile) -> Result<Vec<u8>> {
    let mut buffer = Vec::new();

    file.read_to_end(&mut buffer).await?;

    Ok(buffer)
}

/// Calculate the FFmpeg "-vf" parameter appropriate for scaling the specified video to the specified resolution.
async fn video_scale(
    image_dir: &str,
    path: &str,
    cache_dir: &str,
    size: Size,
    hash: &str,
) -> Result<String> {
    let image = webp::Decoder::new(
        &content(
            &mut thumbnail(None, image_dir, path, cache_dir, size, hash)
                .await?
                .0,
        )
        .await?,
    )
    .decode()
    .ok_or_else(|| anyhow!("invalid WebP image"))?;

    let mut width = image.width();
    let mut height = image.height();

    // The H.264 encoder requires both dimensions to be divisible by 2.

    if width % 2 != 0 {
        width -= 1;
    }

    if height % 2 != 0 {
        height -= 1;
    }

    Ok(format!("scale={width}:{height},setsar=1:1"))
}

/// Return true iff the specified `entry` has an audio codec supported by all modern web browsers.
fn audio_supported(entry: &SampleEntry) -> bool {
    static SUPPORTED_AUDIO_CODECS: &[CodecType] = &[CodecType::AAC, CodecType::MP3];

    if let SampleEntry::Audio(entry) = entry {
        SUPPORTED_AUDIO_CODECS
            .iter()
            .any(|&codec| codec == entry.codec_type)
    } else {
        false
    }
}

/// Represents metadata for the primary track of an MPEG-4 video
struct VideoTrackData {
    /// The width and height of the primary video track
    dimensions: (u32, u32),

    /// The duration of the primary video track
    duration: Duration,

    /// The ID of the primary video track
    id: usize,
}

/// Represents metadata for an MPEG-4 video
struct VideoData {
    /// True iff the file contains only audio tracks which are not known to be supported by all modern web
    /// browsers
    need_audio_transcode: bool,

    /// See `VideoTrackData`
    ///
    /// If this is `None`, then no video track was found in the file.
    track_data: Option<VideoTrackData>,
}

/// Extract relevant metadata from the specified `context`.
///
/// This function tries to identify at least one audio track which is encoded with a codec supported by all modern
/// web browsers, as well as which video track, if any, is the longest, which we consider to be the "primary" video
/// track.
fn get_video_data(context: MediaContext) -> VideoData {
    let mut audio = None;
    let mut video = None;

    for track in context.tracks {
        match track.track_type {
            TrackType::Audio => {
                audio = audio
                    .and_then(|audio: Track| {
                        if let Some(audio_descriptions) = &audio.stsd {
                            if audio_descriptions.descriptions.iter().any(audio_supported)
                                || track.stsd.is_none()
                            {
                                Some(audio)
                            } else {
                                None
                            }
                        } else {
                            None
                        }
                    })
                    .or(Some(track))
            }

            TrackType::Video => {
                video = video
                    .and_then(|video: Track| {
                        if let Some(video_duration) = &video.duration {
                            if let Some(track_duration) = &track.duration {
                                if video_duration.0 >= track_duration.0 {
                                    Some(video)
                                } else {
                                    None
                                }
                            } else {
                                Some(video)
                            }
                        } else {
                            None
                        }
                    })
                    .or(Some(track))
            }

            _ => (),
        }
    }

    VideoData {
        need_audio_transcode: if let Some(audio) = audio {
            if let Some(audio_descriptions) = &audio.stsd {
                !audio_descriptions.descriptions.iter().any(audio_supported)
            } else {
                false
            }
        } else {
            false
        },

        track_data: video.map(|video| VideoTrackData {
            dimensions: video
                .tkhd
                .map(|tkhd| (tkhd.width, tkhd.height))
                .unwrap_or((0, 0)),

            duration: Duration::from_nanos(
                video
                    .duration
                    .zip(video.timescale)
                    .and_then(|(duration, timescale)| {
                        duration.0.checked_mul(1_000_000_000 / timescale.0)
                    })
                    .unwrap_or(0),
            ),

            id: video.id,
        }),
    }
}

/// Async-friendly wrapper for `NamedTempFile`
///
/// See `TempFile::drop` for why this wrapper exists.
pub struct TempFile(pub Option<NamedTempFile>);

impl Drop for TempFile {
    /// Drop the inner `NamedTempFile` inside a `task::block_in_place` context so that Tokio knows this thread may
    /// block on synchronous I/O.
    fn drop(&mut self) {
        task::block_in_place(|| drop(self.0.take()))
    }
}

/// Create a temporary file containing the suffix of the file at `path` starting at `offset` bytes from the
/// beginning.
async fn copy_from_offset(path: &Path, offset: i64) -> Result<TempFile> {
    let mut original = AsyncFile::open(path).await?;

    original
        .seek(SeekFrom::Start(offset.try_into().unwrap()))
        .await?;

    let mut tmp = TempFile(Some(task::block_in_place(NamedTempFile::new)?));

    let mut buffer = vec![0; BUFFER_SIZE];

    loop {
        let count = original.read(&mut buffer[..]).await?;
        if count == 0 {
            break;
        } else {
            task::block_in_place(|| tmp.0.as_mut().unwrap().write_all(&buffer[0..count]))?;
        }
    }

    Ok(tmp)
}

/// Generate a preview version of the specified video at the requested resolution.
///
/// If `size` is `Size::Small`, the clip will be truncated to [VIDEO_PREVIEW_LENGTH_HMS] and resampled to
/// [SMALL_BOUNDS].  If `size` is `Size::Large` will be resampled to [LARGE_BOUNDS] and not truncated.
///
/// The return value is a tuple containing the file handle and length.
async fn video_preview(
    image_lock: &AsyncRwLock<()>,
    image_dir: &str,
    path: &str,
    offset: i64,
    cache_dir: &str,
    size: Size,
    hash: &str,
) -> Result<(AsyncFile, u64)> {
    let filename = format!("{cache_dir}/video/{size}/{hash}.mp4");

    let read = image_lock.read().await;

    let result = AsyncFile::open(&filename).await;

    if let Ok(file) = result {
        let length = file.metadata().await?.len();

        Ok((file, length))
    } else {
        drop(read);

        let _write = image_lock.write().await;

        if let Some(parent) = Path::new(&filename).parent() {
            if let Some(parent) = parent.parent() {
                let _ = fs::create_dir(parent).await;
            }
            let _ = fs::create_dir(parent).await;
        }

        let full_path = [image_dir, path].iter().collect::<PathBuf>();

        let video_data = get_video_data(task::block_in_place(|| {
            let mut file = File::open(&full_path)?;

            file.seek(SeekFrom::Start(offset.try_into()?))?;

            mp4parse::read_mp4(&mut file).map_err(Error::from)
        })?);

        let output = match size {
            Size::Small => {
                let scale = video_scale(image_dir, path, cache_dir, Size::Small, hash).await?;

                if offset == 0 {
                    Command::new("ffmpeg")
                        .arg("-i")
                        .arg(&full_path)
                        .arg("-ss")
                        .arg("00:00:00")
                        .arg("-to")
                        .arg(VIDEO_PREVIEW_LENGTH_HMS)
                        .arg("-vf")
                        .arg(&scale)
                        .arg("-an")
                        .arg(&filename)
                        .output()
                        .await
                } else {
                    let tmp = copy_from_offset(&full_path, offset).await?;

                    let mut command = Command::new("ffmpeg");

                    command.arg("-i").arg(tmp.0.as_ref().unwrap().path());

                    if let Some(track_data) = video_data.track_data {
                        command.arg("-map").arg(format!("0:{}", track_data.id));
                    }

                    command
                        .arg("-vf")
                        .arg(&scale)
                        .arg("-an")
                        .arg(&filename)
                        .output()
                        .await
                }
            }

            Size::Large => {
                if offset == 0 {
                    // Transcode video to reduce file size.
                    //
                    // TODO: We should first check whether the original file is already a reasonable size
                    // (i.e. given its dimensions and duration), in which case we can skip this step and just serve
                    // up the original file.
                    Command::new("ffmpeg")
                        .arg("-i")
                        .arg(&full_path)
                        .arg("-vcodec")
                        .arg("libx264")
                        .arg("-crf")
                        .arg("24") // see https://trac.ffmpeg.org/wiki/Encode/H.264#crf
                        .arg("-acodec")
                        .arg(if video_data.need_audio_transcode {
                            "mp3"
                        } else {
                            "copy"
                        })
                        .arg(&filename)
                        .output()
                        .await
                } else if let Some(track_data) = video_data.track_data {
                    let tmp = copy_from_offset(&full_path, offset).await?;

                    Command::new("ffmpeg")
                        .arg("-i")
                        .arg(tmp.0.as_ref().unwrap().path())
                        .arg("-map")
                        .arg(format!("0:{}", track_data.id))
                        .arg("-vcodec")
                        .arg("copy")
                        .arg("-an")
                        .arg(&filename)
                        .output()
                        .await
                } else {
                    let mut file = AsyncFile::open(&full_path).await?;
                    let offset = offset.try_into().unwrap();
                    let length = file.metadata().await?.len() - offset;

                    file.seek(SeekFrom::Start(offset)).await?;

                    return Ok((file, length));
                }
            }
        }?;

        if output.status.success() {
            let file = AsyncFile::open(&filename).await?;
            let length = file.metadata().await?.len();

            Ok((file, length))
        } else {
            let _ = fs::remove_file(&filename).await;

            Err(anyhow!(
                "error running ffmpeg: {}",
                String::from_utf8_lossy(&output.stderr)
            ))
        }
    }
}

/// Find the still image preview for the specified media item and specified size, generating it from the original
/// if it doesn't already exist.
///
/// The return value is a tuple containing the file handle, length, and filename.
async fn thumbnail(
    image_lock: Option<&AsyncRwLock<()>>,
    image_dir: &str,
    path: &str,
    cache_dir: &str,
    size: Size,
    hash: &str,
) -> Result<(AsyncFile, u64, PathBuf)> {
    let filename = format!("{cache_dir}/{size}/{hash}.webp");

    let read = if let Some(image_lock) = image_lock {
        Some(image_lock.read().await)
    } else {
        None
    };

    let result = AsyncFile::open(&filename).await;

    Ok(if let Ok(file) = result {
        let length = file.metadata().await?.len();

        (file, length, filename.into())
    } else {
        drop(read);

        let _write = if let Some(image_lock) = image_lock {
            Some(image_lock.write().await)
        } else {
            None
        };

        let (image, format) = still_image(image_dir, path).await?;

        let original = image::load_from_memory_with_format(&image, format)?;

        if let Some(parent) = Path::new(&filename).parent() {
            let _ = fs::create_dir(parent).await;
        }

        {
            let orientation = ExifMetadata::new_from_buffer(&image)
                .inspect(|metadata| {
                    let orientation = metadata.get_orientation();
                    metadata.clear();
                    metadata.set_orientation(orientation);
                })
                .as_ref()
                .map(|m| m.get_orientation())
                .unwrap_or(Orientation::Normal);

            let (width, height) = bound(
                original.dimensions(),
                match size {
                    Size::Small => SMALL_BOUNDS,
                    Size::Large => LARGE_BOUNDS,
                },
                orientation,
            );

            task::block_in_place(|| {
                let transformed = original.resize(width, height, FilterType::Lanczos3);

                let transformed = match orientation {
                    Orientation::Rotate90 => transformed.rotate90(),
                    Orientation::Rotate180 => transformed.rotate180(),
                    Orientation::Rotate270 => transformed.rotate270(),
                    Orientation::Normal | Orientation::Unspecified => transformed,
                    _ => return Err(anyhow!("unsupported orientation: {orientation:?}")),
                };

                File::create(&filename)?.write_all(
                    &webp::Encoder::from_image(&transformed)
                        .map_err(|e| anyhow!("{e}"))?
                        .encode(WEBP_QUALITY),
                )?;

                Ok::<_, Error>(())
            })?;
        }

        let file = AsyncFile::open(&filename).await?;
        let length = file.metadata().await?.len();

        (file, length, filename.into())
    })
}

/// Convert the specified `AsyncRead` into a `Stream` of `Bytes`.
///
/// The latter is what Warp needs when sending a binary response which we don't want to load into memory all at
/// once.
fn as_stream(input: impl AsyncRead + Send) -> impl Stream<Item = Result<Bytes>> + Send {
    FramedRead::new(input, BytesCodec::new())
        .map_ok(BytesMut::freeze)
        .map_err(Error::from)
}

/// Handle a GET /image/.. request.
///
/// `hash` specifies which image is requested, while `variant` specifies which version of the image is requested.
///
/// `if_modified_since` and `ranges` enable cache control and HTTP range requests, respectively.  All responses are
/// considered immutable, so if `if_modified_since` is non-empty, we'll send a 304 Not Modified with no further
/// consideration.
#[allow(clippy::too_many_arguments)]
pub async fn image(
    conn: &AsyncMutex<SqliteConnection>,
    image_lock: &AsyncRwLock<()>,
    image_dir: &str,
    cache_dir: &str,
    hash: &str,
    variant: Variant,
    if_modified_since: Option<HttpDate>,
    ranges: Option<&Ranges>,
) -> Result<Response<Body>> {
    if if_modified_since.is_some() {
        return Ok(crate::response()
            .status(StatusCode::NOT_MODIFIED)
            .body(Body::empty())?);
    }

    let file_data = file_data(conn.lock().await.deref_mut(), hash).await?;

    let (mut image, content_type, length) =
        get_variant(image_lock, image_dir, &file_data, cache_dir, variant, hash).await?;

    let cache_control = "public, max-age=31536000, immutable";

    Ok(if let Some(Ranges(ranges)) = ranges {
        if ranges.len() == 1 {
            let range = &ranges[0];

            let (start, end) = match (range.start, range.end) {
                (Some(start), Some(end)) => (start, end + 1),
                (Some(start), None) => (start, length),
                (None, Some(end)) => (length - end, length),
                _ => (0, length),
            };

            if start > length
                || end > length
                || start > end
                || i64::try_from(start).is_err()
                || end == 0
            {
                return Err(HttpError::from_slice(
                    StatusCode::RANGE_NOT_SATISFIABLE,
                    "range not satisfiable",
                )
                .into());
            }

            image
                .seek(SeekFrom::Current(start.try_into().unwrap()))
                .await?;

            crate::response()
                .status(StatusCode::PARTIAL_CONTENT)
                .header(
                    header::CONTENT_RANGE,
                    format!("bytes {start}-{}/{length}", end - 1),
                )
                .header(header::CONTENT_LENGTH, end - start)
                .header(header::CONTENT_TYPE, content_type)
                .header(header::CACHE_CONTROL, cache_control)
                .body(Body::wrap_stream(as_stream(image.take(end - start))))?
        } else {
            // This wouldn't be hard to support, but I don't know if and when any modern web browser makes
            // multi-range requests; no need to implement something if it will never be used.
            return Err(HttpError::from_slice(
                StatusCode::RANGE_NOT_SATISFIABLE,
                "multiple ranges not yet supported",
            )
            .into());
        }
    } else {
        crate::response()
            .header(header::CONTENT_LENGTH, length)
            .header(header::CONTENT_TYPE, content_type)
            .header(header::CACHE_CONTROL, cache_control)
            .body(Body::wrap_stream(as_stream(image)))?
    })
}
